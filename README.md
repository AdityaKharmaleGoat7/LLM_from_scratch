# LLM From Scratch

This repository is where I learn and implement Large Language Models completely from scratch.  
I am following different resources, experimenting with ideas, and slowly building up my understanding of tokenization, embeddings, self-attention, transformers, and text generation.

The goal of this repo is not to create a perfect model, but to document my learning process step by step.  
I will keep adding new code, experiments, and notes as I progress.

### What this repo will include:
- Basic tokenizers (char-level, word-level)
- Bigram and n-gram language models
- Self-attention and multi-head attention
- Transformer blocks
- Training small GPT-like models
- Text generation experiments
- My own notes and explanations

This is a work in progress. More updates coming soon!
